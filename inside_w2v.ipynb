{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1, '../samples/LJ001-0001.wav')"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_path = glob(\"../samples/*.wav\")\n",
    "samples_path = [samples_path[1]]\n",
    "\n",
    "len(samples_path), samples_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"indonesian-nlp/wav2vec2-large-xlsr-indonesian\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"indonesian-nlp/wav2vec2-large-xlsr-indonesian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Wav2Vec2Config {\n  \"_name_or_path\": \"indonesian-nlp/wav2vec2-large-xlsr-indonesian\",\n  \"activation_dropout\": 0.055,\n  \"apply_spec_augment\": true,\n  \"architectures\": [\n    \"Wav2Vec2ForCTC\"\n  ],\n  \"attention_dropout\": 0.094,\n  \"bos_token_id\": 1,\n  \"conv_bias\": true,\n  \"conv_dim\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"conv_kernel\": [\n    10,\n    3,\n    3,\n    3,\n    3,\n    2,\n    2\n  ],\n  \"conv_stride\": [\n    5,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2\n  ],\n  \"ctc_loss_reduction\": \"mean\",\n  \"ctc_zero_infinity\": true,\n  \"do_stable_layer_norm\": true,\n  \"eos_token_id\": 2,\n  \"feat_extract_activation\": \"gelu\",\n  \"feat_extract_dropout\": 0.0,\n  \"feat_extract_norm\": \"layer\",\n  \"feat_proj_dropout\": 0.04,\n  \"final_dropout\": 0.0,\n  \"gradient_checkpointing\": true,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.047,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-05,\n  \"layerdrop\": 0.041,\n  \"mask_channel_length\": 10,\n  \"mask_channel_min_space\": 1,\n  \"mask_channel_other\": 0.0,\n  \"mask_channel_prob\": 0.0,\n  \"mask_channel_selection\": \"static\",\n  \"mask_feature_length\": 10,\n  \"mask_feature_prob\": 0.0,\n  \"mask_time_length\": 10,\n  \"mask_time_min_space\": 1,\n  \"mask_time_other\": 0.0,\n  \"mask_time_prob\": 0.4,\n  \"mask_time_selection\": \"static\",\n  \"model_type\": \"wav2vec2\",\n  \"num_attention_heads\": 16,\n  \"num_conv_pos_embedding_groups\": 16,\n  \"num_conv_pos_embeddings\": 128,\n  \"num_feat_extract_layers\": 7,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 27,\n  \"transformers_version\": \"4.6.0.dev0\",\n  \"vocab_size\": 28\n}"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params            : 315,467,420\n",
      "Feature Encoder Params  : 4,210,176\n",
      "Feature Projector Params: 526,336\n",
      "Encoder Params          : 310,701,184\n",
      "Classifier Head Params  : 28,700\n"
     ]
    }
   ],
   "source": [
    "total_param = sum(p.numel() for p in model.parameters())\n",
    "fe_param = sum(p.numel() for p in model.wav2vec2.feature_extractor.parameters())\n",
    "fp_param = sum(p.numel() for p in model.wav2vec2.feature_projection.parameters())\n",
    "enc_param = sum(p.numel() for p in model.wav2vec2.encoder.parameters())\n",
    "cls_param = sum(p.numel() for p in model.lm_head.parameters())\n",
    "\n",
    "print(f\"Total Params            : {total_param:,}\")\n",
    "print(f\"Feature Encoder Params  : {fe_param:,}\")\n",
    "print(f\"Feature Projector Params: {fp_param:,}\")\n",
    "print(f\"Encoder Params          : {enc_param:,}\")\n",
    "print(f\"Classifier Head Params  : {cls_param:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 188624])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_wave(path):\n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, 16_000)\n",
    "    wave = resampler(speech_array).squeeze().numpy()\n",
    "    return wave\n",
    "\n",
    "samples = [read_wave(path) for path in samples_path]\n",
    "inputs = processor(samples, sampling_rate = 16_000, return_tensors = \"pt\", padding = True)\n",
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs.input_values, attention_mask = inputs.attention_mask, output_hidden_states = True)\n",
    "    logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 589, 28])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 589])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['percetakan dalam satusatunya pengertian yang menjadi perhatian kita saat ini berbeda dari sebagian besar jika tidak semua seni dan kerajinan yang diwakili dalam pameran']"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ids = torch.argmax(logits, dim = -1)\n",
    "pred_tokens = processor.batch_decode(pred_ids)\n",
    "pred_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state 0 torch.Size([1, 589, 1024])\n",
      "hidden state 1 torch.Size([1, 589, 1024])\n",
      "hidden state 2 torch.Size([1, 589, 1024])\n",
      "hidden state 3 torch.Size([1, 589, 1024])\n",
      "hidden state 4 torch.Size([1, 589, 1024])\n",
      "hidden state 5 torch.Size([1, 589, 1024])\n",
      "hidden state 6 torch.Size([1, 589, 1024])\n",
      "hidden state 7 torch.Size([1, 589, 1024])\n",
      "hidden state 8 torch.Size([1, 589, 1024])\n",
      "hidden state 9 torch.Size([1, 589, 1024])\n",
      "hidden state 10 torch.Size([1, 589, 1024])\n",
      "hidden state 11 torch.Size([1, 589, 1024])\n",
      "hidden state 12 torch.Size([1, 589, 1024])\n",
      "hidden state 13 torch.Size([1, 589, 1024])\n",
      "hidden state 14 torch.Size([1, 589, 1024])\n",
      "hidden state 15 torch.Size([1, 589, 1024])\n",
      "hidden state 16 torch.Size([1, 589, 1024])\n",
      "hidden state 17 torch.Size([1, 589, 1024])\n",
      "hidden state 18 torch.Size([1, 589, 1024])\n",
      "hidden state 19 torch.Size([1, 589, 1024])\n",
      "hidden state 20 torch.Size([1, 589, 1024])\n",
      "hidden state 21 torch.Size([1, 589, 1024])\n",
      "hidden state 22 torch.Size([1, 589, 1024])\n",
      "hidden state 23 torch.Size([1, 589, 1024])\n",
      "hidden state 24 torch.Size([1, 589, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i, h in enumerate(outputs.hidden_states):\n",
    "    print(f\"hidden state {i} {h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Wav2Vec2FeatureExtractor(\n  (conv_layers): ModuleList(\n    (0): Wav2Vec2LayerNormConvLayer(\n      (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): Wav2Vec2LayerNormConvLayer(\n      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): Wav2Vec2LayerNormConvLayer(\n      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): Wav2Vec2LayerNormConvLayer(\n      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): Wav2Vec2LayerNormConvLayer(\n      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): Wav2Vec2LayerNormConvLayer(\n      (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (6): Wav2Vec2LayerNormConvLayer(\n      (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wav2vec2.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hugsy': conda)",
   "name": "hugsy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}